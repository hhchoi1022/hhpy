{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtcWIgsmy7tp"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxWlaZZYy-Bz"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJfJrLohzCRB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change directory to where this file is located\n",
    "\"\"\"\n",
    "\n",
    "# %cd 'COPY&PASTE FILE DIRECTORY HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ht5_qm33z74C"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import PIL\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmNbnxILz8W6"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(\"Using PyTorch version: {}, Device: {}\".format(torch.__version__, DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nQb58d6yNMg"
   },
   "source": [
    "# Practice 1. Dataset and DataLoader (Visual Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6FFS5rp3QTj"
   },
   "source": [
    "## Question1. Explore CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwHj0Zzwy60v"
   },
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(root=\"./CIFAR_10\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVkA_UDD3BAy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(1) Print the 1st element of the cifar10 train dataset.\n",
    "\"\"\"\n",
    "\n",
    "########## Your Code #########\n",
    "\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNGDL-SKyL-i"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(2) Visualize the 2nd element of the cifar10 train dataset with its label.\n",
    "\n",
    "  - hint1: Use plt.imshow() function to visualize the image.\n",
    "  - hint2: Use CIFAR10_LABEL to extract the label. With the given label index in the dataset, you can extract the label.\n",
    "\"\"\"\n",
    "\n",
    "CIFAR10_LABEL = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "########## Your Code #########\n",
    "\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t2J-Co65Hk5"
   },
   "source": [
    "## Question2. Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBmcEX-F5KiE"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(1) Implement customized dataset for CIFAR10\n",
    "\n",
    "\"\"\"\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train, data_dir=\"./CIFAR_10\"):\n",
    "        \"\"\"Initializes the dataset.\n",
    "        \n",
    "        - loads CIFAR10 dataset to self.data\n",
    "\n",
    "        Args:\n",
    "          train: If set to True, train dataset will be used.\n",
    "          data_dir: Directory to download CIFAR10 dataset\n",
    "        \"\"\"\n",
    "        ########## Your Code #########\n",
    "        self.data = None\n",
    "        ##############################\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of self.data.\n",
    "        \"\"\"\n",
    "        ########## Your Code #########\n",
    "        return None\n",
    "        ##############################\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns data point with the given index.\n",
    "        \"\"\"\n",
    "        ########## Your Code #########\n",
    "        return None\n",
    "        ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5d7JccO6Tri"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train=True)\n",
    "test_dataset = CustomDataset(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhED9t-P6aRT"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(2) Convert the first 4 samples in the train_dataset to pytorch tensor and print their sizes and labels.\n",
    "\n",
    "  - hint1: Use transforms.ToTensor() to convert image to pytorch tensor.\n",
    "  - hint2: label information is given with CIFAR10_LABEL variable.\n",
    "\"\"\"\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(4):\n",
    "    print(f\"<Image no.{i}>\")\n",
    "    image, label_idx = train_dataset[i]\n",
    "\n",
    "    ########## Your Code #########\n",
    "\n",
    "    ##############################\n",
    "    print(image.shape)\n",
    "    \n",
    "    ########## Your Code #########\n",
    "\n",
    "    ##############################\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoDTjDPV9RCE"
   },
   "source": [
    "## Question3. collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xbw7MZt9WdN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(1) Implement the function collate_fn.\n",
    "\"\"\"\n",
    "\n",
    "def collate_fn(data_samples):\n",
    "    \"\"\"Takes in the input samples from the Dataset, and makes them into a batch for the DataLoader.\n",
    "\n",
    "    - Convert the PIL images in data_samples to pytorch tensors using ToTensor().\n",
    "    - Convert the labels in data_samples to pytorch tensors.\n",
    "      (hint : You should put the label in a list before transforming it into a tensor)\n",
    "    - Append preprocessed image and label tensors to batch_x and batch_y respectively.\n",
    "    - Convert the batch of image tensors into PyTorch float tensors.\n",
    "    - Convert the batch of labels into PyTorch long tensors.\n",
    "      (hint : Refer to the shapes of the Returns below)\n",
    "\n",
    "    Args:\n",
    "      data_samples: list of tuples, each containing a PIL image and an integer label\n",
    "\n",
    "    Returns:\n",
    "      batch_x: batch of image tensors. size: (BATCH, CHANNEL, HEIGHT, WIDTH)\n",
    "      batch_y: batch of label tensors. size: (BATCH)\n",
    "    \"\"\"\n",
    "    batch_x, batch_y = [], []\n",
    "    ########## Your Code #########\n",
    "\n",
    "    ##############################\n",
    "    return (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tx_S2qpEl2sw"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6qsv-g_m79r"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1Z2Yjwy9bS9"
   },
   "source": [
    "## Question4. Transforms (Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rO5ggJlqtNU"
   },
   "outputs": [],
   "source": [
    "def visualize_batch(batch, augment=None):\n",
    "    images, labels = batch\n",
    "    batch_size = images.shape[0]\n",
    "    pltsize = 2\n",
    "    plt.figure(figsize=(batch_size * pltsize, pltsize))\n",
    "    for i in range(batch_size):\n",
    "        plt.subplot(1, batch_size, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(np.transpose(augment(images[i]) if augment else images[i], (1, 2, 0)))\n",
    "        plt.title('Class: ' + str(CIFAR10_LABEL[labels[i].item()]))\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "visualize_batch(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqah2mmzrDrU"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(1) Apply data augmentations using torchvision.transforms module.\n",
    "\n",
    "- refererence: https://pytorch.org/vision/master/transforms.html\n",
    "\"\"\"\n",
    "\n",
    "########## Your Code #########\n",
    "# This is the sample code. Try different augmentation methods!\n",
    "visualize_batch(sample_batch, augment=transforms.Grayscale(num_output_channels=3))\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbTNloKxt4zv"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(2) Implement __call__(self, image) function in the RandomAugmenation class.\n",
    "\"\"\"\n",
    "\n",
    "augment_pool = [\n",
    "    # Try other augmentation methods as well!\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.functional.hflip,\n",
    "    transforms.functional.vflip,\n",
    "]\n",
    "\n",
    "\n",
    "class RandomAugmentation:\n",
    "    def __init__(self, augment_pool, prob=0.5):\n",
    "        self.augment_pool = augment_pool\n",
    "        self.prob = prob\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        \"\"\"Applies stochastic data augmentation to the given image.\n",
    "\n",
    "        - Randomly choose the augmentation method from the augment_pool.\n",
    "        - Random augmentation is applied with the probability of prob.\n",
    "        - If prob = 0.5, 5 out of 10 images will be augmented on average.\n",
    "\n",
    "        Args:\n",
    "        image: An input image\n",
    "        augment_pool: augmentation methods to apply\n",
    "        prob: probability to apply augmentation\n",
    "\n",
    "        Returns:\n",
    "        image: augmented input image    \n",
    "        \"\"\"\n",
    "        ########## Your Code #########\n",
    "\n",
    "        ##############################\n",
    "        return image\n",
    "\n",
    "\n",
    "visualize_batch(sample_batch, augment=RandomAugmentation(augment_pool, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjLrda1KtgQy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(3) Redefine customized dataset for CIFAR10\n",
    "\n",
    "\"\"\"\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train, transform=None, data_dir=\"./CIFAR_10\"):\n",
    "        \"\"\"Initializes the dataset.\n",
    "        \n",
    "        - loads CIFAR10 dataset to self.data\n",
    "        - assign transform to self.transform\n",
    "\n",
    "        Args:\n",
    "          train: If set to True, train dataset will be used.\n",
    "          transform: If given, it will be applied to images when collate_fn is called.\n",
    "          data_dir: Directory to download CIFAR10 dataset\n",
    "        \"\"\"\n",
    "        ########## Your Code #########\n",
    "        self.data = None\n",
    "        self.transform = None\n",
    "        ##############################\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of self.data.\n",
    "        \"\"\"\n",
    "        ########## Your Code #########\n",
    "        return None\n",
    "        ##############################\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns data point with the given index.\n",
    "        \"\"\"\n",
    "        ########## Your Code #########\n",
    "        return None\n",
    "        ##############################\n",
    "\n",
    "    def collate_fn(self, data_samples):\n",
    "        \"\"\"Takes in the input samples from the Dataset, and makes them into a batch for the DataLoader.\n",
    "\n",
    "        - Convert the PIL images in data_samples to pytorch tensors using ToTensor().\n",
    "        - Transform the image using self.transform method to apply random augmentation.\n",
    "        - Convert the labels in data_samples to pytorch tensors.\n",
    "        - Append preprocessed image and label tensors to batch_x and batch_y respectively.\n",
    "        - Convert the batch of image tensors into PyTorch float tensors.\n",
    "        - Convert the batch of labels into PyTorch long tensors.\n",
    "        \"\"\"\n",
    "        batch_x, batch_y = [], []\n",
    "        ########## Your Code #########\n",
    "\n",
    "        ##############################    \n",
    "        return (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rV-s6c1qshu"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train=True, transform=RandomAugmentation(augment_pool, 0.5))\n",
    "test_dataset = CustomDataset(train=False, transform=RandomAugmentation(augment_pool, 0.5))\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=train_dataset.collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "visualize_batch(sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZRYJDkQyUDC"
   },
   "source": [
    "# Practice 2. Train the Model (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZUsgO69-nke"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train=True, transform=None)\n",
    "test_dataset = CustomDataset(train=False, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iz_85LepCvj_"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    tqdm_bar = tqdm(train_loader)\n",
    "    for batch_idx, (image, label) in enumerate(tqdm_bar):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        prediction = output.max(1, keepdim = True)[1]\n",
    "        correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "        optimizer.step()\n",
    "        tqdm_bar.set_description(\"Epoch {} - train loss: {:.6f}\".format(epoch, loss.item()))\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMtjfCozCwNV"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, return_samples=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    random_idx = np.random.randint(len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, label) in enumerate(tqdm(test_loader)):\n",
    "            if batch_idx == random_idx:\n",
    "                samples = (image, label)\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    return (test_loss, test_acc, samples) if return_samples else (test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJlJ9zx-40Eh"
   },
   "source": [
    "## Question1. Multi Layer Perceptron\n",
    "\n",
    "Perform architecture search to find the best MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLoH3n6ryVCE"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(1) Define your own model (model hyperparameters)\n",
    "\n",
    "  - which layers to use? how many layers to use?\n",
    "  - which activation function to use?\n",
    "\"\"\"\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        ########## Your Code #########\n",
    "        # This is the sample code. Define your own model!\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        ##############################    \n",
    "\n",
    "    def forward(self, x):\n",
    "        ########## Your Code #########\n",
    "        # This is the sample code. Define your own model!\n",
    "        x = x.view(-1, 32 * 32 * 3)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        ##############################    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XDAlHt6-d5N"
   },
   "outputs": [],
   "source": [
    "model = MLP().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKmyr23i5zHu"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(2) Hyperparameter Tuning (training hyperparameters)\n",
    "\n",
    "  - search different learning rates\n",
    "  - try different batch size\n",
    "  - try different optimizers\n",
    "\"\"\"\n",
    "\n",
    "########## Your Code #########\n",
    "# This is the sample code. Try different hyperparameters!\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "##############################    \n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=train_dataset.collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tModel: MLP, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9F7Ihx848oS"
   },
   "source": [
    "## Question2. Convolutional Neural Network\n",
    "\n",
    "Perform architecture search to find the best MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVA7l2ua5AU3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(1) Define your own model (model hyperparameters)\n",
    "\n",
    "  - how many convolution, pooling, and fully connected layers to use?\n",
    "  - how many channels to use in each convolution layer?\n",
    "  - find the kernel size, padding, and strides with the best performance\n",
    "  - which activation function to use?\n",
    "\"\"\"\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        ########## Your Code #########\n",
    "        # This is the sample code. Define your own model!\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(8 * 16 * 16, 10),\n",
    "        )\n",
    "        ##############################    \n",
    "                \n",
    "    def forward(self, x):\n",
    "        ########## Your Code #########\n",
    "        # This is the sample code. Define your own model!\n",
    "        x = self.conv(x)\n",
    "        ##############################    \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVvEYHrX_Gsl"
   },
   "outputs": [],
   "source": [
    "model = CNN().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-RG3Z6U702d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(2) Hyperparameter Tuning (training hyperparameters)\n",
    "\n",
    "  - search different learning rates\n",
    "  - try different batch size\n",
    "  - try different optimizers\n",
    "\"\"\"\n",
    "\n",
    "########## Your Code #########\n",
    "# This is the sample code. Try different hyperparameters!\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "##############################    \n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=train_dataset.collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tModel: CNN, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qnm14MYx8DCq"
   },
   "source": [
    "## Question3. Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiuk_9Rk_aaF"
   },
   "outputs": [],
   "source": [
    "model = CNN().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErpDSQPV_gF5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(1) Hyperparameter Tuning (training hyperparameters)\n",
    "\n",
    "  - search different learning rates\n",
    "  - try different batch size\n",
    "  - try different optimizers\n",
    "\"\"\"\n",
    "\n",
    "########## Your Code #########\n",
    "# This is the sample code. Try different hyperparameters!\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "##############################    \n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=train_dataset.collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53r-QBkZE33R"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(2) Try different learning rate scheduler\n",
    "\n",
    "  - reference: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "\"\"\"\n",
    "\n",
    "########## Your Code #########\n",
    "# This is the sample code. Try different learning rate schedulers!\n",
    "from torch.optim.lr_scheduler import ExponentialLR as ExponentialLR\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "##############################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTcyy0liBy5h"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    curr_lr = scheduler.get_last_lr()[0]\n",
    "    train(model, train_loader, optimizer, scheduler)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tModel: CNN, \\tLR: {:.4f}, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, curr_lr, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlmAhS7oFibj"
   },
   "source": [
    "# Practice3. TensorBoard\n",
    "\n",
    "- https://pytorch.org/docs/stable/tensorboard.html\n",
    "- https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mzFnLfzAZVG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr6Rv_-g97yV"
   },
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.moveaxis(npimg, 0, -1)    \n",
    "    plt.imshow(npimg)\n",
    "\n",
    "def images_to_probs(model, images):\n",
    "    output = model(images.to(DEVICE))\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.cpu().numpy())\n",
    "    return preds, [F.softmax(out, dim=0)[pred_idx].item() for pred_idx, out in zip(preds, output)]\n",
    "\n",
    "def plot_classes_preds(model, images, labels):\n",
    "    preds, probs = images_to_probs(model, images)\n",
    "    fig = plt.figure(figsize=(12, 24))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx])\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            CIFAR10_LABEL[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            CIFAR10_LABEL[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKukgKR8HD5n"
   },
   "source": [
    "## 1. TensorBoard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOJ3k4moD7Se"
   },
   "outputs": [],
   "source": [
    "log_dir = \"./runs/MLDL1_Lab2\"\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l6LPBMZIos8"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfYI6hi-HeuY"
   },
   "source": [
    "## 2. Tracking model training with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDFTgjm4PkRI"
   },
   "outputs": [],
   "source": [
    "model = CNN().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=train_dataset.collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tqJ02eW9bcI"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    curr_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, scheduler)\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", train_accuracy, epoch)\n",
    "    \n",
    "    test_loss, test_accuracy, samples = evaluate(model, test_loader, return_samples=True)\n",
    "    writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/test\", test_accuracy, epoch)\n",
    "\n",
    "    images, labels = samples\n",
    "    writer.add_figure(\"Predictions vs Actuals\",\n",
    "                      plot_classes_preds(model, images, labels))\n",
    "    \n",
    "    print(\"\\n[EPOCH: {}], \\tModel: CNN, \\tLR: {:.4f}, \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, curr_lr, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sb5SNRrCStK9"
   },
   "outputs": [],
   "source": [
    "hparam_dict = {\n",
    "    \"lr\": learning_rate,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "metric_dict = {\n",
    "    \"accuracy\": test_accuracy\n",
    "}\n",
    "\n",
    "writer.add_hparams(hparam_dict, metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0V78Bnk_2lJ"
   },
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yj09QHy4AkQP"
   },
   "outputs": [],
   "source": [
    "%ls ./runs/MLDL1_Lab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtXt1lufAWPp"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./runs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
